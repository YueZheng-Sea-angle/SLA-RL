"""
å…ƒè®¤çŸ¥æ¡†æ¶åŒ…è£…å™¨ v4 - æ¿€è¿›ç‰ˆ
================================
æ ¸å¿ƒå‡çº§ï¼šä»"æ¸©å’Œé•‡é™å‰‚"åˆ°"å¼ºåŠ›åŠ é€Ÿå™¨"

å…³é”®ä¿®å¤ï¼š
1. åŸºäºZ-Scoreçš„æ‰¹æ¬¡å†…å½’ä¸€åŒ– â†’ é«˜æ–¹å·®æƒé‡åˆ†å¸ƒ
2. éçº¿æ€§æ”¾å¤§å›°éš¾æ ·æœ¬ â†’ çœŸæ­£çš„å¥½å¥‡å¿ƒåŠ é€Ÿ
3. æ ·æœ¬çº§åˆ«æƒé‡ â†’ ç²¾ç»†æ§åˆ¶
"""

import torch
import numpy as np
from curiosity_evaluator import SimplifiedCuriosityEvaluator


class AggressiveMetaWrapper:
    """
    æ¿€è¿›ç‰ˆå…ƒè®¤çŸ¥åŒ…è£…å™¨
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - âœ“ Z-Scoreå½’ä¸€åŒ–ï¼šä¿è¯æ‰¹æ¬¡å†…é«˜æ–¹å·®
    - âœ“ æ ·æœ¬çº§æƒé‡ï¼šç²¾ç»†åˆ°æ¯ä¸ªæ ·æœ¬
    - âœ“ éçº¿æ€§æ”¾å¤§ï¼šå›°éš¾æ ·æœ¬æŒ‡æ•°çº§å…³æ³¨
    """
    
    def __init__(self, base_algorithm, state_dim, action_dim, 
                 meta_lr=1e-3, warmup_steps=2000, 
                 scale_factor=0.5, use_exponential=False,
                 device='cpu'):
        """
        Args:
            scale_factor: æƒé‡åˆ†å¸ƒçš„æ¿€è¿›ç¨‹åº¦ (0.3-1.0)
                - 0.3: æ¸©å’Œ (æƒé‡èŒƒå›´ [0.7, 1.3])
                - 0.5: ä¸­ç­‰ (æƒé‡èŒƒå›´ [0.5, 1.5])
                - 1.0: æ¿€è¿› (æƒé‡èŒƒå›´ [0.0, 2.0])
            use_exponential: æ˜¯å¦ä½¿ç”¨æŒ‡æ•°æ”¾å¤§
        """
        self.base_algorithm = base_algorithm
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.warmup_steps = warmup_steps
        self.scale_factor = scale_factor
        self.use_exponential = use_exponential
        
        # å¥½å¥‡å¿ƒè¯„ä»·å™¨
        self.curiosity_evaluator = SimplifiedCuriosityEvaluator(
            state_dim=state_dim,
            action_dim=1 if not hasattr(base_algorithm, 'actor') else action_dim,
            hidden_dim=64,
            lr=meta_lr,
            device=device
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.total_updates = 0
        self.episode_rewards = []
        
        # TD Errorè¿½è¸ª
        self.td_error_history = []
        self.running_max_td_error = 1.0  # è¿è¡Œæœ€å¤§TDè¯¯å·®ï¼ˆç”¨äºæ–¹æ¡ˆBï¼‰
        self.running_avg_td_error = 1.0  # è¿è¡Œå¹³å‡ï¼ˆç”¨äºç›‘æ§ï¼‰
        
        # æƒé‡ç»Ÿè®¡ï¼ˆç”¨äºç›‘æ§ï¼‰
        self.weight_history = []
        
    def select_action(self, state, eval_mode=False):
        return self.base_algorithm.select_action(state, eval_mode)
    
    def store_transition(self, state, action, reward, next_state, done):
        self.base_algorithm.store_transition(state, action, reward, next_state, done)
    
    def add_episode_reward(self, reward):
        """è®°å½•å›åˆå¥–åŠ±"""
        self.episode_rewards.append(reward)
        if len(self.episode_rewards) > 100:
            self.episode_rewards.pop(0)
    
    def _compute_sample_td_errors(self, batch):
        """
        è®¡ç®—æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„TD Error
        
        Returns:
            td_errors: [batch_size] æ¯ä¸ªæ ·æœ¬çš„TDè¯¯å·®
        """
        states, actions, rewards, next_states, dones = batch
        
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        with torch.no_grad():
            # å½“å‰Qå€¼
            q_values = self.base_algorithm.q_net(states)
            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)
            
            # ç›®æ ‡Qå€¼
            if hasattr(self.base_algorithm, 'use_double_dqn') and self.base_algorithm.use_double_dqn:
                next_actions = self.base_algorithm.q_net(next_states).argmax(1)
                next_q_values = self.base_algorithm.target_q_net(next_states).gather(
                    1, next_actions.unsqueeze(1)).squeeze(1)
            else:
                next_q_values = self.base_algorithm.target_q_net(next_states).max(1)[0]
            
            target_q_values = rewards + (1 - dones) * self.base_algorithm.gamma * next_q_values
            
            # TD Errorï¼ˆç»å¯¹å€¼ï¼‰
            td_errors = torch.abs(q_values - target_q_values)
        
        return td_errors
    
    def _compute_target_weights_zscore(self, batch_td_errors):
        """
        ğŸ¯ æ–¹æ¡ˆA: åŸºäºZ-Scoreçš„å¼ºåŠ›å½’ä¸€åŒ–
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - åœ¨æ‰¹æ¬¡å†…éƒ¨åˆ¶é€ é«˜å¯¹æ¯”åº¦
        - é«˜TD Erroræ ·æœ¬ â†’ w > 1 (åŠ å¼ºå­¦ä¹ )
        - ä½TD Erroræ ·æœ¬ â†’ w < 1 (å¿«é€Ÿè·³è¿‡)
        - å¹³å‡æƒé‡ â‰ˆ 1.0 (ä¸æ”¹å˜æ•´ä½“å­¦ä¹ ç‡)
        
        Args:
            batch_td_errors: [batch_size] TDè¯¯å·®
            
        Returns:
            target_weights: [batch_size] ç›®æ ‡æƒé‡
        """
        # 1. è®¡ç®—æ‰¹æ¬¡ç»Ÿè®¡é‡
        mean = batch_td_errors.mean()
        std = batch_td_errors.std() + 1e-6
        
        # 2. Z-Scoreæ ‡å‡†åŒ–
        # Zå€¼é€šå¸¸åœ¨[-2, 2]èŒƒå›´å†…
        z_scores = (batch_td_errors - mean) / std
        
        # 3. æ˜ å°„åˆ°æƒé‡
        # scale_factoræ§åˆ¶æ¿€è¿›ç¨‹åº¦
        # scale_factor=0.5: z=2æ—¶w=2.0, z=-2æ—¶w=0.0
        # scale_factor=1.0: z=1æ—¶w=2.0, z=-1æ—¶w=0.0
        target_weights = 1.0 + z_scores * self.scale_factor
        
        # 4. è£å‰ªåˆ°å®‰å…¨èŒƒå›´
        target_weights = torch.clamp(target_weights, 0.1, 2.0)
        
        return target_weights
    
    def _compute_target_weights_exponential(self, batch_td_errors):
        """
        ğŸ¯ æ–¹æ¡ˆB: éçº¿æ€§æ”¾å¤§ï¼ˆå¥½å¥‡å¿ƒåŠ é€Ÿï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - å¯¹é«˜TD Erroræ ·æœ¬ç»™äºˆæŒ‡æ•°çº§å¥–åŠ±
        - æ¨¡æ‹ŸçœŸæ­£çš„"å¥½å¥‡å¿ƒ"é©±åŠ¨
        
        Args:
            batch_td_errors: [batch_size] TDè¯¯å·®
            
        Returns:
            target_weights: [batch_size] ç›®æ ‡æƒé‡
        """
        # å½’ä¸€åŒ–åˆ°[0, 1]
        normalized = batch_td_errors / (self.running_max_td_error + 1e-6)
        normalized = torch.clamp(normalized, 0, 1)
        
        # æ›´æ–°è¿è¡Œæœ€å¤§å€¼
        current_max = batch_td_errors.max().item()
        self.running_max_td_error = 0.99 * self.running_max_td_error + 0.01 * current_max
        
        # æŒ‡æ•°çº§å…³æ³¨å›°éš¾æ ·æœ¬
        # normalized=1.0 (æœ€å¤§è¯¯å·®) â†’ w=2.0
        # normalized=0.5 â†’ w=1.25
        # normalized=0.0 â†’ w=1.0
        target_weights = 1.0 + (normalized ** 2)
        
        # è£å‰ª
        target_weights = torch.clamp(target_weights, 0.5, 2.0)
        
        return target_weights
    
    def _compute_target_weights_hybrid(self, batch_td_errors):
        """
        ğŸ¯ æ–¹æ¡ˆC: æ··åˆç­–ç•¥ï¼ˆZ-Score + æŒ‡æ•°æ”¾å¤§ï¼‰
        
        ç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼š
        - Z-Scoreä¿è¯æ‰¹æ¬¡å†…å¯¹æ¯”åº¦
        - æŒ‡æ•°æ”¾å¤§ç»™å›°éš¾æ ·æœ¬é¢å¤–å¥–åŠ±
        """
        # 1. Z-ScoreåŸºç¡€æƒé‡
        mean = batch_td_errors.mean()
        std = batch_td_errors.std() + 1e-6
        z_scores = (batch_td_errors - mean) / std
        base_weights = 1.0 + z_scores * 0.4  # ç¨å¾®ä¿å®ˆä¸€ç‚¹
        
        # 2. æŒ‡æ•°åŠ æˆï¼ˆç»™çœŸæ­£å›°éš¾çš„æ ·æœ¬é¢å¤–boostï¼‰
        normalized = batch_td_errors / (self.running_max_td_error + 1e-6)
        normalized = torch.clamp(normalized, 0, 1)
        curiosity_bonus = 0.3 * (normalized ** 2)  # æœ€å¤š+0.3
        
        # 3. ç»„åˆ
        target_weights = base_weights + curiosity_bonus
        
        # 4. è£å‰ª
        target_weights = torch.clamp(target_weights, 0.1, 2.0)
        
        return target_weights
    
    def update(self, batch_size=64):
        """
        ğŸš€ æ ·æœ¬çº§åˆ«çš„å…ƒè®¤çŸ¥å¢å¼ºæ›´æ–°
        
        Returns:
            dict: è¯¦ç»†çš„æ›´æ–°ç»Ÿè®¡
        """
        if len(self.base_algorithm.replay_buffer) < batch_size:
            return {
                'base_loss': 0,
                'meta_loss': 0,
                'avg_weight': 1.0,
                'weight_std': 0,
                'in_warmup': True
            }
        
        self.total_updates += 1
        
        # WarmupæœŸ
        if self.total_updates < self.warmup_steps:
            base_loss, avg_td = self.base_algorithm.update(batch_size)
            return {
                'base_loss': base_loss,
                'meta_loss': 0,
                'avg_weight': 1.0,
                'weight_std': 0,
                'td_error': avg_td,
                'in_warmup': True
            }
        
        # æ¸è¿›å¼å¼•å…¥å…ƒè®¤çŸ¥
        progress = min(1.0, (self.total_updates - self.warmup_steps) / 2000)
        
        # 1. é‡‡æ ·æ‰¹æ¬¡
        batch = self.base_algorithm.replay_buffer.sample(batch_size)
        
        # 2. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„TD Error
        sample_td_errors = self._compute_sample_td_errors(batch)
        
        # 3. è®¡ç®—ç›®æ ‡æƒé‡ï¼ˆä½¿ç”¨é€‰å®šçš„ç­–ç•¥ï¼‰
        if self.use_exponential:
            target_weights = self._compute_target_weights_exponential(sample_td_errors)
        else:
            target_weights = self._compute_target_weights_zscore(sample_td_errors)
        
        # å¯é€‰ï¼šä½¿ç”¨æ··åˆç­–ç•¥
        # target_weights = self._compute_target_weights_hybrid(sample_td_errors)
        
        # 4. æ­£å¸¸æ›´æ–°baseç®—æ³•ï¼ˆä¸åŠ æƒï¼Œç”¨äºè·å–åŸºç¡€æŸå¤±ï¼‰
        base_loss, avg_td = self.base_algorithm.update(batch_size, weight=None)
        
        # 5. è®­ç»ƒå…ƒè¯„ä»·å™¨
        meta_loss = 0
        pred_weights = target_weights  # åˆå§‹åŒ–
        
        if self.total_updates > self.warmup_steps + 500:  # ç»™ç‚¹é¢å¤–warmup
            states = torch.FloatTensor(batch[0]).to(self.device)
            actions = torch.FloatTensor(batch[1]).to(self.device)
            if actions.dim() == 1:
                actions = actions.unsqueeze(1)
            
            # è®­ç»ƒè¯„ä»·å™¨æ‹Ÿåˆç›®æ ‡æƒé‡
            meta_loss = self.curiosity_evaluator.update(states, actions, target_weights)
            
            # è·å–é¢„æµ‹æƒé‡
            pred_weights = self.curiosity_evaluator.evaluate_batch_value(states, actions)
            
            # æ¸è¿›å¼åº”ç”¨ï¼ˆåæœŸå¯ä»¥å®Œå…¨ä½¿ç”¨é¢„æµ‹æƒé‡ï¼‰
            if progress < 0.5:
                # æ—©æœŸï¼šä¸»è¦ç”¨ç›®æ ‡æƒé‡
                final_weights = target_weights * (1 - progress * 2) + pred_weights * (progress * 2)
            else:
                # åæœŸï¼šä¸»è¦ç”¨é¢„æµ‹æƒé‡
                final_weights = pred_weights
        else:
            final_weights = target_weights
        
        # 6. è®°å½•ç»Ÿè®¡
        avg_weight = final_weights.mean().item()
        weight_std = final_weights.std().item()
        self.weight_history.append({'mean': avg_weight, 'std': weight_std})
        
        self.td_error_history.append(avg_td)
        if len(self.td_error_history) > 1000:
            self.td_error_history.pop(0)
        
        self.running_avg_td_error = 0.99 * self.running_avg_td_error + 0.01 * avg_td
        
        return {
            'base_loss': base_loss,
            'meta_loss': meta_loss,
            'avg_weight': avg_weight,
            'weight_std': weight_std,
            'weight_min': final_weights.min().item(),
            'weight_max': final_weights.max().item(),
            'td_error': avg_td,
            'target_weight_mean': target_weights.mean().item(),
            'target_weight_std': target_weights.std().item(),
            'in_warmup': False,
            'progress': progress
        }
    
    def get_stats(self):
        """è·å–è¯¦ç»†ç»Ÿè®¡"""
        stats = {
            'total_updates': self.total_updates,
            'in_warmup': self.total_updates < self.warmup_steps,
            'num_episodes': len(self.episode_rewards),
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'avg_td_error': np.mean(self.td_error_history) if self.td_error_history else 0,
        }
        
        if len(self.weight_history) > 0:
            recent_weights = self.weight_history[-100:]
            stats['avg_weight_mean'] = np.mean([w['mean'] for w in recent_weights])
            stats['avg_weight_std'] = np.mean([w['std'] for w in recent_weights])
        
        return stats


class UltraAggressiveMetaWrapper(AggressiveMetaWrapper):
    """
    è¶…æ¿€è¿›ç‰ˆæœ¬ - è¿›ä¸€æ­¥æ”¾å¤§æƒé‡å·®å¼‚
    
    é€‚ç”¨åœºæ™¯ï¼š
    - Baseç®—æ³•å·²ç»å¾ˆç¨³å®š
    - æƒ³è¦æœ€å¤§åŒ–å…ƒè®¤çŸ¥çš„å½±å“
    - æ„¿æ„æ‰¿æ‹…ä¸€å®šé£é™©
    """
    
    def __init__(self, base_algorithm, state_dim, action_dim, 
                 meta_lr=1e-3, warmup_steps=2000, device='cpu'):
        super().__init__(
            base_algorithm=base_algorithm,
            state_dim=state_dim,
            action_dim=action_dim,
            meta_lr=meta_lr,
            warmup_steps=warmup_steps,
            scale_factor=1.0,  # æ›´æ¿€è¿›ï¼
            use_exponential=False,
            device=device
        )
    
    def _compute_target_weights_zscore(self, batch_td_errors):
        """è¶…æ¿€è¿›ç‰ˆZ-Score"""
        mean = batch_td_errors.mean()
        std = batch_td_errors.std() + 1e-6
        z_scores = (batch_td_errors - mean) / std
        
        # æ›´æ¿€è¿›çš„æ˜ å°„
        # z=2 â†’ w=2.0, z=-2 â†’ w=0.0
        target_weights = 1.0 + z_scores * 1.0  # scale_factor=1.0
        
        # ç¨å¾®å®½æ¾çš„è£å‰ª
        target_weights = torch.clamp(target_weights, 0.05, 2.0)
        
        return target_weights

