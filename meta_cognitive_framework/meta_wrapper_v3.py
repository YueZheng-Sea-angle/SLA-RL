"""
å…ƒè®¤çŸ¥æ¡†æ¶åŒ…è£…å™¨ v3 - ç»ˆæä¿®å¤ç‰ˆ
================================
æ ¸å¿ƒä¿®å¤ï¼š"ä»¥æƒŠå¥‡åº¦è®ºè‹±é›„"è€Œé"ä»¥ç»“æœè®ºè‹±é›„"
"""

import torch
import numpy as np
from curiosity_evaluator import SimplifiedCuriosityEvaluator


class SurpriseDrivenMetaWrapper:
    """
    æƒŠå¥‡é©±åŠ¨çš„å…ƒè®¤çŸ¥åŒ…è£…å™¨
    
    æ ¸å¿ƒç†å¿µè½¬å˜ï¼š
    - æ—§ï¼šPerformance â†“ â†’ w â†“ (æ™´å¤©é€ä¼ï¼Œé›¨å¤©æ”¶ä¼) âŒ
    - æ–°ï¼šTD Error â†‘ â†’ w â†‘ (ä¿¡æ¯é‡å¤§çš„æ ·æœ¬ä¼˜å…ˆå­¦ä¹ ) âœ“
    
    çµæ„Ÿæ¥æºï¼šPrioritized Experience Replay (PER)
    """
    
    def __init__(self, base_algorithm, state_dim, action_dim, 
                 meta_lr=1e-3, warmup_steps=2000, device='cpu'):
        """
        Args:
            warmup_steps: é¢„çƒ­æ­¥æ•°ï¼ˆè®©baseç®—æ³•å…ˆç¨³å®šï¼‰
        """
        self.base_algorithm = base_algorithm
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.warmup_steps = warmup_steps
        
        # å¥½å¥‡å¿ƒè¯„ä»·å™¨
        self.curiosity_evaluator = SimplifiedCuriosityEvaluator(
            state_dim=state_dim,
            action_dim=1 if not hasattr(base_algorithm, 'actor') else action_dim,
            hidden_dim=64,
            lr=meta_lr,
            device=device
        )
        
        # è®­ç»ƒç»Ÿè®¡
        self.total_updates = 0
        self.episode_rewards = []
        
        # ğŸ”§ æ–°å¢ï¼šè¿½è¸ªTD Errorå’ŒLoss
        self.td_error_history = []
        self.running_avg_td_error = 1.0  # è¿è¡Œå¹³å‡TDè¯¯å·®
        self.running_avg_loss = 1.0      # è¿è¡Œå¹³å‡æŸå¤±
        
        # æ€§èƒ½è¿½è¸ªï¼ˆè¾…åŠ©ä¿¡æ¯ï¼‰
        self.performance_window = []
        
    def select_action(self, state, eval_mode=False):
        return self.base_algorithm.select_action(state, eval_mode)
    
    def store_transition(self, state, action, reward, next_state, done):
        self.base_algorithm.store_transition(state, action, reward, next_state, done)
    
    def add_episode_reward(self, reward):
        """è®°å½•å›åˆå¥–åŠ±"""
        self.episode_rewards.append(reward)
        self.performance_window.append(reward)
        
        if len(self.performance_window) > 20:
            self.performance_window.pop(0)
        if len(self.episode_rewards) > 100:
            self.episode_rewards.pop(0)
    
    def _compute_target_weight_v3(self, td_error, current_performance):
        """
        ğŸ¯ æ ¸å¿ƒä¿®å¤ï¼šåŸºäº"æƒŠå¥‡åº¦"è€Œé"ç»“æœ"
        
        æ–°é€»è¾‘ï¼š
        1. é«˜TD Error = é«˜ä¿¡æ¯é‡ = åº”è¯¥å¤šå­¦ä¹ 
        2. ä½†è¦é˜²æ­¢å¼‚å¸¸å€¼ç ´åè®­ç»ƒ
        3. ç»“åˆæ€§èƒ½è¶‹åŠ¿ä½œä¸ºè¾…åŠ©
        
        Args:
            td_error: å½“å‰æ‰¹æ¬¡çš„TDè¯¯å·®
            current_performance: æœ€è¿‘çš„æ€§èƒ½ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            target_w: ç›®æ ‡æƒé‡ [0.5, 2.0]
        """
        # å½’ä¸€åŒ–TD Error
        normalized_td = td_error / (self.running_avg_td_error + 1e-6)
        
        # æ›´æ–°è¿è¡Œå¹³å‡ï¼ˆç¼“æ…¢æ›´æ–°ï¼‰
        self.running_avg_td_error = 0.99 * self.running_avg_td_error + 0.01 * td_error
        
        # ğŸ”§ æ–¹æ¡ˆ A: PERé£æ ¼ - ç®€å•æœ‰æ•ˆ
        # TD Error è¶Šå¤§ï¼Œæƒé‡è¶Šé«˜
        # ä½¿ç”¨tanhå¹³æ»‘ï¼Œé¿å…æç«¯å€¼
        surprise_bonus = 0.5 * np.tanh(normalized_td - 1.0)
        target_w = 1.0 + surprise_bonus
        
        # é™åˆ¶èŒƒå›´
        target_w = np.clip(target_w, 0.5, 2.0)
        
        return target_w
    
    def _compute_target_weight_v3_advanced(self, td_error, batch_loss, current_performance):
        """
        ğŸ¯ é«˜çº§ç‰ˆæœ¬ï¼šæ··åˆé€»è¾‘
        
        è€ƒè™‘ä¸‰ä¸ªå› ç´ ï¼š
        1. TD Error (ä¿¡æ¯é‡)
        2. Loss (å­¦ä¹ éš¾åº¦)
        3. Performance (ç»“æœè´¨é‡)
        """
        # 1. å½’ä¸€åŒ–TD Error
        normalized_td = td_error / (self.running_avg_td_error + 1e-6)
        self.running_avg_td_error = 0.99 * self.running_avg_td_error + 0.01 * td_error
        
        # 2. å½’ä¸€åŒ–Loss
        normalized_loss = batch_loss / (self.running_avg_loss + 1e-6)
        self.running_avg_loss = 0.99 * self.running_avg_loss + 0.01 * batch_loss
        
        # 3. è®¡ç®—æ€§èƒ½è¶‹åŠ¿
        if len(self.performance_window) >= 10:
            recent_perf = np.mean(self.performance_window[-5:])
            older_perf = np.mean(self.performance_window[-10:-5])
            perf_trend = (recent_perf - older_perf) / (abs(older_perf) + 1)
        else:
            perf_trend = 0
        
        # ğŸ¯ æ··åˆå†³ç­–é€»è¾‘
        # 
        # æƒ…å†µ1: é«˜TD Error + æ€§èƒ½è¿˜è¡Œ â†’ é«˜ä»·å€¼æ ·æœ¬ (High Leverage)
        #   - è¿™äº›æ ·æœ¬ä¿¡æ¯é‡å¤§ï¼Œä¸”ä¸æ˜¯çº¯å™ªéŸ³
        #   - w = 1.5 - 2.0
        #
        # æƒ…å†µ2: é«˜TD Error + æ€§èƒ½å¾ˆå·® â†’ å¯èƒ½å¤ªéš¾æˆ–å™ªéŸ³
        #   - å…ˆä¸æ€¥ç€å­¦ï¼Œæˆ–è€…é€‚åº¦å­¦ä¹ 
        #   - w = 0.8 - 1.2
        #
        # æƒ…å†µ3: ä½TD Error â†’ å·²ç»å­¦ä¼šäº†
        #   - é™ä½æƒé‡ï¼ŒèŠ‚çœè®¡ç®—/é˜²æ­¢è¿‡æ‹Ÿåˆ
        #   - w = 0.5 - 0.8
        
        # åŸºç¡€æƒé‡ï¼šåŸºäºTD Error
        base_w = 1.0 + 0.5 * np.tanh(normalized_td - 1.0)
        
        # è°ƒæ•´ï¼šåŸºäºæ€§èƒ½è¶‹åŠ¿
        if normalized_td > 1.5:  # é«˜TD Error
            if perf_trend > 0:  # æ€§èƒ½åœ¨ä¸Šå‡
                # é«˜æ æ†æ ·æœ¬ï¼åŠ å¼ºå­¦ä¹ 
                adjustment = 0.3
            else:  # æ€§èƒ½åœ¨ä¸‹é™
                # å¯èƒ½æ˜¯éš¾æ ·æœ¬æˆ–å™ªéŸ³ï¼Œé€‚åº¦å­¦ä¹ 
                adjustment = -0.1
        else:  # ä½TD Error
            # å·²ç»å­¦ä¼šçš„æ ·æœ¬ï¼Œé™ä½æƒé‡
            adjustment = -0.2
        
        target_w = base_w + adjustment
        
        # é™åˆ¶èŒƒå›´
        target_w = np.clip(target_w, 0.5, 2.0)
        
        return target_w
    
    def update(self, batch_size=64):
        """
        ğŸš€ å…ƒè®¤çŸ¥å¢å¼ºçš„æ›´æ–°æµç¨‹
        
        Returns:
            dict: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.base_algorithm.replay_buffer) < batch_size:
            return {
                'base_loss': 0, 
                'meta_loss': 0, 
                'weight': 1.0, 
                'td_error': 0,
                'in_warmup': True
            }
        
        self.total_updates += 1
        
        # WarmupæœŸï¼šåªæ›´æ–°baseç®—æ³•
        if self.total_updates < self.warmup_steps:
            base_loss, td_error = self.base_algorithm.update(batch_size)
            return {
                'base_loss': base_loss,
                'meta_loss': 0,
                'weight': 1.0,
                'td_error': td_error,
                'in_warmup': True
            }
        
        # æ¸è¿›å¼å¼•å…¥å…ƒè®¤çŸ¥
        progress = min(1.0, (self.total_updates - self.warmup_steps) / 2000)
        
        # 1. ç¬¬ä¸€æ¬¡æ›´æ–°ï¼šè·å–TD Errorï¼ˆä¸åº”ç”¨æƒé‡ï¼‰
        base_loss_1, td_error = self.base_algorithm.update(batch_size, weight=None)
        
        # 2. è®¡ç®—ç›®æ ‡æƒé‡ï¼ˆåŸºäºTD Errorï¼‰
        current_perf = np.mean(self.performance_window[-5:]) if len(self.performance_window) >= 5 else 0
        
        # ä½¿ç”¨ç®€å•ç‰ˆæœ¬ï¼ˆPERé£æ ¼ï¼‰
        target_w = self._compute_target_weight_v3(td_error, current_perf)
        
        # 3. è®­ç»ƒå…ƒè¯„ä»·å™¨
        meta_loss = 0
        pred_weight = 1.0
        
        if len(self.episode_rewards) > 5:  # æœ‰è¶³å¤Ÿæ•°æ®
            # é‡‡æ ·æ‰¹æ¬¡
            batch = self.base_algorithm.replay_buffer.sample(batch_size)
            states = torch.FloatTensor(batch[0]).to(self.device)
            actions = torch.FloatTensor(batch[1]).to(self.device)
            if actions.dim() == 1:
                actions = actions.unsqueeze(1)
            
            # ç›®æ ‡æƒé‡ï¼ˆæ‰€æœ‰æ ·æœ¬ç”¨åŒä¸€ä¸ªæƒé‡ï¼Œç®€åŒ–ç‰ˆï¼‰
            target_weights = torch.full((batch_size,), target_w, device=self.device)
            
            # æ›´æ–°å…ƒè¯„ä»·å™¨
            meta_loss = self.curiosity_evaluator.update(states, actions, target_weights)
            
            # è·å–é¢„æµ‹æƒé‡
            pred_weights = self.curiosity_evaluator.evaluate_batch_value(states, actions)
            pred_weight = pred_weights.mean().item()
            
            # æ¸è¿›å¼åº”ç”¨
            pred_weight = 1.0 * (1 - progress) + pred_weight * progress
        
        # è®°å½•TD Error
        self.td_error_history.append(td_error)
        if len(self.td_error_history) > 1000:
            self.td_error_history.pop(0)
        
        return {
            'base_loss': base_loss_1,
            'meta_loss': meta_loss,
            'weight': pred_weight,
            'target_weight': target_w,
            'td_error': td_error,
            'normalized_td': td_error / (self.running_avg_td_error + 1e-6),
            'in_warmup': False,
            'progress': progress
        }
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_updates': self.total_updates,
            'in_warmup': self.total_updates < self.warmup_steps,
            'num_episodes': len(self.episode_rewards),
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'avg_td_error': np.mean(self.td_error_history) if self.td_error_history else 0,
            'running_avg_td': self.running_avg_td_error
        }

